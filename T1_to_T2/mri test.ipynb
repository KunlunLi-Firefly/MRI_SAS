{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPNnMZcGG/u9S38+1FTWKBf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ed_8MOOoTE5_","executionInfo":{"status":"ok","timestamp":1715024354705,"user_tz":240,"elapsed":19307,"user":{"displayName":"Peiqi Shi","userId":"13648918649974290420"}},"outputId":"0c5b043e-fb48-4a54-ca2c-6c4de0bc2ab7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","\n"]},{"cell_type":"code","source":["\n","import tensorflow as tf\n","import numpy as np\n","from PIL import Image\n","import glob\n","import os\n","from skimage.transform import resize\n","import matplotlib.pyplot as plt\n","\n","# Instance Normalization class as used in your model\n","class InstanceNormalization(tf.keras.layers.Layer):\n","    def __init__(self, epsilon=1e-5):\n","        super(InstanceNormalization, self).__init__()\n","        self.epsilon = epsilon\n","\n","    def build(self, input_shape):\n","        self.scale = self.add_weight(name='scale', shape=input_shape[-1:],\n","                                     initializer=tf.random_normal_initializer(1., 0.02),\n","                                     trainable=True)\n","        self.offset = self.add_weight(name='offset', shape=input_shape[-1:],\n","                                      initializer='zeros', trainable=True)\n","\n","    def call(self, x, training=False):\n","        mean, variance = tf.nn.moments(x, axes=[1, 2], keepdims=True)\n","        inv = tf.math.rsqrt(variance + self.epsilon)\n","        normalized = (x - mean) * inv\n","        return self.scale * normalized + self.offset\n","\n","def downsample(filters, size, apply_norm=True):\n","    initializer = tf.random_normal_initializer(0., 0.02)\n","    down_layers = tf.keras.Sequential()\n","    #adding Conv2D layer\n","    down_layers.add(tf.keras.layers.Conv2D(filters, size, strides = 2, padding = 'same',\n","                                        kernel_initializer = initializer, use_bias = False))\n","    #adding normalization\n","    if apply_norm:\n","        down_layers.add(InstanceNormalization())\n","    #adding activation function as Leaky Relu\n","    down_layers.add(tf.keras.layers.LeakyReLU())\n","    return down_layers\n","\n","\n","def upsample(filters, size, apply_dropout=False):\n","    initializer = tf.random_normal_initializer(0., 0.02)\n","    up_layers = tf.keras.Sequential()\n","    # Add Transposed Conv2d layer\n","    up_layers.add(tf.keras.layers.Conv2DTranspose(filters, size, strides=2, padding='same',\n","                                               kernel_initializer=initializer, use_bias=False))\n","    # Add Normalization Layer\n","    up_layers.add(InstanceNormalization())\n","    # Conditionally add Dropout layer\n","    if apply_dropout:\n","        up_layers.add(tf.keras.layers.Dropout(0.5))\n","    # Add Relu Activation Layer\n","    up_layers.add(tf.keras.layers.ReLU())\n","    return up_layers\n","\n","\n","def unet_generator():\n","    down_stack = [\n","        downsample(64, 4, False), #(bs, 128, 128, 64)\n","        downsample(128, 4), #(bs, 64, 64, 128)\n","        downsample(256, 4), #(bs, 32, 32, 256)\n","        downsample(512, 4), #(bs, 16, 16, 512)\n","        downsample(512, 4), #(bs, 8, 8, 512)\n","        downsample(512, 4), #(bs, 4, 4, 512)\n","        downsample(512, 4), #(bs, 2, 2, 512)\n","        downsample(512, 4) #(bs, 1, 1, 512)\n","    ]\n","\n","    up_stack = [\n","        upsample(512, 4, apply_dropout=True),  # (bs, 2, 2, 1024)\n","        upsample(512, 4, apply_dropout=True),  # (bs, 4, 4, 1024)\n","        upsample(512, 4, apply_dropout=True),  # (bs, 8, 8, 1024)\n","        upsample(512, 4),  # (bs, 16, 16, 1024)\n","        upsample(256, 4),  # (bs, 32, 32, 512)\n","        upsample(128, 4),  # (bs, 64, 64, 256)\n","        upsample(64, 4),  # (bs, 128, 128, 128)\n","    ]\n","    initializer = tf.random_normal_initializer(0., 0.02)\n","    last = tf.keras.layers.Conv2DTranspose(1, 4,\n","                                         strides=2,\n","                                         padding='same',\n","                                         kernel_initializer=initializer,\n","                                         activation='tanh')  # (bs, 256, 256, 1)\n","    inputs = tf.keras.layers.Input(shape=[256, 256, 1])\n","    concat = tf.keras.layers.Concatenate()\n","    x = inputs\n","\n","    # Downsampling through the model\n","    skips = []\n","    for down in down_stack:\n","        x = down(x)\n","        skips.append(x)\n","\n","    skips = reversed(skips[:-1])\n","\n","  # Upsampling and establishing the skip connections\n","    for up, skip in zip(up_stack, skips):\n","        x = up(x)\n","        x = concat([x, skip])\n","\n","    x = last(x)\n","\n","    return tf.keras.Model(inputs=inputs, outputs=x)\n","\n","# Load the pre-trained model\n","model = unet_generator()\n","checkpoint_path = '/content/drive/My Drive/mia/trainedmodel2'\n","ckpt = tf.train.Checkpoint(generator_g=model)\n","status = ckpt.restore(tf.train.latest_checkpoint(checkpoint_path)).expect_partial()  # Using expect_partial to ignore optimizer states\n","\n","# Function to process new images and save them\n","def process_and_save_images(input_dir, output_dir):\n","    if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","    all_t1_images = glob.glob(f\"{input_dir}/*.png\")\n","    for filename in all_t1_images:\n","        img = Image.open(filename).convert('L')\n","        img = img.resize((256, 256))\n","        img_array = np.array(img) / 127.5 - 1\n","        img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n","        img_array = np.expand_dims(img_array, axis=-1)  # Add channel dimension\n","        prediction = model(img_array, training=False)\n","        prediction = (prediction[0, :, :, 0].numpy() + 1) * 127.5  # Rescale to 0-255\n","        prediction = np.clip(prediction, 0, 255).astype(np.uint8)\n","        result_path = os.path.join(output_dir, os.path.basename(filename))\n","        Image.fromarray(prediction).save(result_path)\n","\n","# Example usage\n","input_directory = '/content/drive/My Drive/mia/mri_taskc/taskc_slices2/subject_61_t1w.nii'\n","output_directory = '/content/drive/My Drive/mia/mri_taskc/taskc_slices2_2/subject_61_t1w.nii'\n","process_and_save_images(input_directory, output_directory)\n"],"metadata":{"id":"tTtgPrlSdJP3","executionInfo":{"status":"ok","timestamp":1715024841038,"user_tz":240,"elapsed":24246,"user":{"displayName":"Peiqi Shi","userId":"13648918649974290420"}}},"execution_count":10,"outputs":[]}]}